<!-- ai-security.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>AI Security — ObfusLabs</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body {
      background-color: #0a0a0a;
      color: #f0f0f0;
      font-family: 'Courier New', Courier, monospace;
      padding: 3rem;
      line-height: 1.6;
      max-width: 980px;
      margin-left: auto;
      margin-right: auto;
    }
    a { color: #00FF00; text-decoration: underline; }
    .logo { font-size: 2.2rem; letter-spacing: 0.6rem; display:flex; margin-bottom:1rem; }
    .char { display:inline-block; transition: color 0.3s; }
    .char.red { color: #FF2B63; }
    header { margin-bottom: 1rem; }
    .post {
      background: rgba(255,255,255,0.02);
      border: 1px solid rgba(255,255,255,0.03);
      padding: 1.5rem;
    }
    .meta { color:#9a9a9a; font-size:0.95rem; margin-bottom:1rem; }
    .hero-img {
      width:100%;
      height:320px;
      background: linear-gradient(90deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01));
      display:flex;
      align-items:center;
      justify-content:center;
      color:#7a7a7a;
      border: 1px dashed rgba(255,255,255,0.03);
      margin: 0.8rem 0 1.25rem 0;
      text-align:center;
      padding: 0 1rem;
    }
    .content h2 { margin-top:0.25rem; }
    .small { font-size:0.9rem; color:#bfbfbf; }
    .note { color:#bfbfbf; font-size:0.95rem; margin-top:1rem; }
    .back { margin-top:1rem; display:inline-block; }
    .callout {
      background: rgba(255,255,255,0.03);
      border: 1px dashed rgba(255,255,255,0.08);
      padding: 0.9rem;
      margin: 1rem 0;
      color:#cfd3cf;
    }
    .grid {
      display:grid;
      grid-template-columns: 1fr;
      gap: 1rem;
    }
    @media (min-width: 820px) {
      .grid { grid-template-columns: 1.2fr 0.8fr; }
    }
    .badge {
      display:inline-block;
      padding: 0.2rem 0.5rem;
      border-radius: 4px;
      background: rgba(0,255,0,0.06);
      color:#bfffbf;
      font-size:0.85rem;
      margin-left:0.5rem;
    }
    .list-tight { margin:0.5rem 0 0.5rem 1.1rem; }
    .repo { display:inline-block; margin-top:0.4rem; }
  </style>
</head>
<body>
  <div class="logo" aria-hidden="true">
    <span class="char">O</span>
    <span class="char red">B</span>
    <span class="char">F</span>
    <span class="char" id="glitch-letter">Ʞ</span>
    <span class="char">S</span>
    <span class="char">L</span>
    <span class="char">Δ</span>
    <span class="char red">B</span>
    <span class="char">S</span>
  </div>

  <header>
    <h1>AI Security</h1>
    <div class="meta">CS50AI completion • adversarial ML, model misuse, and autonomy risk through a red-team lens</div>
  </header>

  <article class="post">
    <div class="meta"><strong>Date:</strong> Sep 15, 2025 <span class="small">• Foundations finished; security experiments ongoing</span></div>

    <div class="hero-img" id="image-placeholder">
     <img src="ai-pic.png" alt="Neural-net / adversarial example visual" />
    </div>

    <section class="content grid">
      <div>
        <h2>Summary</h2>
        <p class="note">
          We completed <em>CS50’s Introduction to Artificial Intelligence with Python</em> and are applying that foundation to practical AI security:
          prompt injection & jailbreaks, data poisoning, adversarial examples, and safe deployment patterns. Our stance is simple:
          treat models like production software with an attack surface — then try to break them.
        </p>

        <h2>Immediate surface (now → 2–5 years)</h2>
        <ul class="list-tight small">
          <li><strong>Prompt injection & jailbreaks:</strong> untrusted content steering model behavior; cross-app “indirect” prompt attacks via tools/RAG.</li>
          <li><strong>Data leakage via embeddings & fine-tuning:</strong> unintentional disclosure of secrets, PII, or proprietary info.</li>
          <li><strong>Data poisoning & model supply chain:</strong> tampered training sets, malicious weights, dependency risks.</li>
          <li><strong>Adversarial examples:</strong> tiny perturbations that flip labels; physical attacks (stickers, patches) against vision models.</li>
          <li><strong>Operational controls:</strong> auth to model endpoints, rate-limits, logging, review, and sandboxed tool use.</li>
        </ul>

        <div class="callout">
          <strong>What we test:</strong> can untrusted inputs hijack the model or tools? can we exfiltrate secrets from embeddings? can small perturbations flip a decision?
        </div>

        <h2>Near-term autonomy risk (5–15 years)</h2>
        <p class="small">
          The “killer robot” concern isn’t sci-fi — the software logic is trivial, hardware cost is the bottleneck.
          As autonomous drones/UGVs become cheaper, the insider-threat / misuse problem becomes a policy and security emergency.
          Our focus here is <em>control</em>: authorization, geofencing, remote shutdown, and preventing model-driven escalation.
        </p>

        <h2>Longer-term (and why we still care)</h2>
        <p class="small">
          General intelligence and recursive self-improvement are hard to scope operationally today, but our day-job defenses (verification,
          monitoring, containment, least-privilege, red-teaming) are the same muscles we’ll need if the capabilities curve bends faster.
        </p>

        <h2>What we finished</h2>
        <ul class="list-tight small">
          <li>CS50AI core topics: search, logic, probability, optimization, neural networks, reinforcement learning.</li>
          <li>All programming assignments and projects are public.</li>
        </ul>
        <p class="small">Repo: <a class="repo" href="https://github.com/exitvillain/harvard-artificial-intelligence" target="_blank" rel="noopener noreferrer">github.com/exitvillain/harvard-artificial-intelligence</a></p>

        <h2>What we’re building next</h2>
        <ul class="list-tight small">
          <li>Minimal prompt-injection test harness for RAG/tool-using agents (attack strings + expected defenses).</li>
          <li>Tiny adversarial-example demo (image classifier flip; physical printable patch optional).</li>
          <li>Embedding-leak lab: measure semantic drift and secret retrieval risks from vector stores.</li>
          <li>Ops hardening checklist for ML endpoints (authN/Z, quotas, content filters, review gates, kill-switches).</li>
        </ul>

        <h2>A tiny promise</h2>
        <p class="note">
          We’ll work to harden deployed models — and if robots start acting less friendly than they should, we plan to know which switch to try first.
        </p>

        <p class="back"><a href="current-research.html">← Back to Current Research</a></p>
      </div>

      <aside>
        <h3>Foundations (short note)</h3>
        <p class="small">
          Embeddings (e.g., word2vec) showed how meaning can live in geometry — similar words cluster in vector space.
          Modern transformers scale this idea massively. We keep this here not as nostalgia, but as the conceptual bridge
          to why adversarial perturbations and poisoning work at all.
        </p>

        <h3>Show, don’t tell</h3>
        <ul class="list-tight small">
          <li>Prefer runnable artifacts to certificate photos.</li>
          <li>Short writeups, clear PoCs, tight scope.</li>
        </ul>

        <h3>Image slot</h3>
        <p class="small">Swap the placeholder with a neural-net diagram, adversarial stop sign, or a training curve screenshot.</p>
      </aside>
    </section>
  </article>

  <footer style="margin-top:2rem; color:#7a7a7a;">
    <p>Questions or collabs: <a href="mailto:daniel.gray@obfuslabs.com">daniel.gray@obfuslabs.com</a></p>
  </footer>

  <script>
    const glitch = document.getElementById('glitch-letter');
    const states = ['█', 'ʞ', 'U'];
    let i = 0;
    setInterval(() => {
      glitch.textContent = states[i % states.length];
      i++;
    }, 700);
  </script>
</body>
</html>

